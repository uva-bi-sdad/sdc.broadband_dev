{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDAD Broadbandnow.com Scraper Code\n",
    "### Last Edit: 2/8/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires addresses from Corelogic/similar source at the block level with 2 important columns: geoid_blk and mail_address. Original approach uses one address per block group and first generates cleaned list of addresses and block groups from this. Then, proceeds to scrape all package information for those selected addresses. Reports results at block group, tract, and county level within dataframe, which can be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# generic imports\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# selenium imports\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if xpath exists, if not return false\n",
    "def check_exists_by_xpath(driver, xpath):\n",
    "    '''\n",
    "    Description:\n",
    "        Check existence of xpath on page\n",
    "    \n",
    "    Inputs:\n",
    "        webdriver: your webdriver\n",
    "        xpath: whatever element we are looking for\n",
    "        \n",
    "    Outputs:\n",
    "        returns True if xpath exists, False if not\n",
    "    '''\n",
    "    # try to find element\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    \n",
    "    # throw exception and return false if unable to find\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Corelogic provides BLOCK level FIPs code, but we use BLOCK GROUP level data here\n",
    "def read_and_clean_addresses_for_bgs(data, need_subset = True, size_subset = 3):\n",
    "    '''\n",
    "    Description:\n",
    "        Check existence of xpath on page\n",
    "    \n",
    "    Inputs:\n",
    "        data: string, name of csv you want to use (includes .csv ending)\n",
    "        need_subset: boolean, True if using subset of data (originally used 1st address within each bg from list of 3) - default = True\n",
    "        size_subset: integer, if subsetting, selects every \"nth\" row (not necessary to mess with this param if using 1 address per bg) - default = 3\n",
    "        \n",
    "    Outputs:\n",
    "        returns True if xpath exists, False if not\n",
    "    '''\n",
    "    # read in csv, drop index, and update block column\n",
    "    address_sample_3_per_bg = pd.read_csv(data, index_col = 0)\n",
    "    address_sample_3_per_bg = address_sample_3_per_bg.reset_index(drop = True)\n",
    "    address_sample_3_per_bg['geoid_blk'] = address_sample_3_per_bg.geoid_blk.astype(str)\n",
    "    \n",
    "    # drop lat 4 digits of mail address to get short zipcode\n",
    "    a = address_sample_3_per_bg.mail_address.values\n",
    "    a = np.array([a[i][0:-4] if a[i][-9].isdigit() else a[i] for i in range(len(a))])\n",
    "    \n",
    "    # get block group geoid\n",
    "    address_sample_3_per_bg['geoid_bg'] = address_sample_3_per_bg.geoid_blk.str.slice(start=0, stop=12)\n",
    "    \n",
    "    # if data needs subsetting (I had 3 addresses )\n",
    "    if need_subset:\n",
    "        addresses = a[::size_subset]\n",
    "        block_geoids = address_sample_3_per_bg.geoid_bg[::size_subset]\n",
    "        \n",
    "    else:\n",
    "        addresses = a\n",
    "        block_geoid = address_sample_3_per_bg.geoid_bg\n",
    "    \n",
    "    return addresses, block_geoids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_address2(address, driver, driver_wait = 20):\n",
    "    '''\n",
    "    Description:\n",
    "        Check existence of xpath on page\n",
    "    \n",
    "    Inputs:\n",
    "        address: string, single home address we are scraping for\n",
    "        driver: your webdriver\n",
    "        driver_wait: integer, wait time for driver - default = 20\n",
    "        \n",
    "    Outputs:\n",
    "        returns True if xpath exists, False if not\n",
    "    '''\n",
    "    # wait until search bar is clickable and enter address\n",
    "    wait = WebDriverWait(driver, driver_wait)\n",
    "    search = wait.until(EC.element_to_be_clickable((By.ID, 'plan-search')))\n",
    "    search.clear()\n",
    "    search.send_keys(\"{}\".format(address))\n",
    "\n",
    "    # sleep, then go to top suggested address\n",
    "    time.sleep(sleep_time)\n",
    "    go_top = check_exists_by_xpath(driver, '//*[@id=\"plans-search\"]/div/div/div[1]/div/div/div/ul')\n",
    "\n",
    "    # click top address\n",
    "    if go_top:\n",
    "        go_top_address = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"plans-search\"]/div/div/div[1]/div/div/div/ul/li')))\n",
    "        go_top_address.click()\n",
    "        \n",
    "    return go_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_prices(driver, addresses):\n",
    "    '''\n",
    "    Description:\n",
    "        Scrape internet packages from Broadbandnow.com - takes each address and scrapes all packages for top match\n",
    "    \n",
    "    Inputs:\n",
    "        driver: your webdriver\n",
    "        addresses: array of strings, home addresses we are scraping for (first output of read_and_clean_addresses_for_bgs)\n",
    "        \n",
    "    Outputs:\n",
    "        all_prices: jagged list (list of varying sized lists), package prices\n",
    "        all_names: jagged list (same size as all_prices), package names\n",
    "        all_type_list: jagged list (same size as all_prices), package names\n",
    "        all_speeds: jagged list (same size as all_prices), package names\n",
    "        idxs: array, array of indices where information was successfully scraped (aligns with addresses)\n",
    "    '''\n",
    "    # create empty lists for prices, names, speeds, and types - will become jagged lists (lists of varying sized lists)\n",
    "    all_prices = []\n",
    "    all_names = []\n",
    "    all_speeds = []\n",
    "    all_type_list = []\n",
    "    idxs = []\n",
    "\n",
    "    # initialize variables and get start time\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    problem_counter = 0\n",
    "\n",
    "    # loop over block group addressed\n",
    "    while i < len(addresses):\n",
    "        # try below and exception IF takes too long (increments a counter before skipping address eventually)\n",
    "        try:\n",
    "            # reload page to clear results (noticed that we run into issues if we do not clear)\n",
    "            driver.get(\"https://broadbandnow.com/compare/plans\")\n",
    "            go_top = search_address2(addresses[i], driver)\n",
    "\n",
    "            # select top address\n",
    "            if go_top:\n",
    "                time.sleep(1)\n",
    "                unable_to_confirm = check_exists_by_xpath(driver, \"/html/body/div[2]/div/div/div[1]/section/section/div/div/div[1]/div/section\")\n",
    "\n",
    "                # if able to confirm and go to top address\n",
    "                if not unable_to_confirm:\n",
    "                    #\n",
    "                    time.sleep(1)\n",
    "                    load_more = check_exists_by_xpath(driver, '//*[@id=\"cityPlansListing\"]/section/div/div[2]/div/div/section')\n",
    "\n",
    "                    #if load more is an option, then load all packages\n",
    "                    if load_more:\n",
    "                        # load all plans\n",
    "                        load_all_plans = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"cityPlansListing\"]/section/div/div[2]/div/div/section')))\n",
    "                        load_all_plans.click()\n",
    "\n",
    "                    # bs - scrape page\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html)\n",
    "\n",
    "                    # extract list of prices\n",
    "                    price_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__plan-value\"})\n",
    "                    price = np.array([float(price_temp_list[i].getText().split(\"$\")[-1]) for i in range(len(price_temp_list))])\n",
    "\n",
    "                    # extract list of name of provider\n",
    "                    name_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__provider-name\"})\n",
    "                    name = np.array([name_temp_list[i].getText().split(\". \")[1] for i in range(len(name_temp_list))])\n",
    "\n",
    "                    # extract list of speeds\n",
    "                    speed_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__speeds-value\"})\n",
    "                    speed = np.array([float(speed_temp_list[i].getText().split(\" \")[0]) for i in range(len(speed_temp_list))])\n",
    "\n",
    "                    # extract string - \"Upload\" or \"Download\"\n",
    "                    down_up_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__speeds-label\"})\n",
    "                    down_up = np.array([down_up_temp_list[i].getText() for i in range(len(speed_temp_list))])\n",
    "\n",
    "                    # extract type of internet service\n",
    "                    type_temp_list = soup.find_all(attrs = {\"class\": \"c-provider-card__label\"})\n",
    "                    type_list = np.array([type_temp_list[i].getText().strip() for i in range(len(type_temp_list))])\n",
    "\n",
    "                    # create empty 2D array for speeds \n",
    "                    speed_array = np.zeros([np.sum(down_up == \"Download\"), 2]) * np.nan\n",
    "                    \n",
    "                    # set counter to 0, will denote the row in speed_array we are filling in\n",
    "                    count = 0\n",
    "\n",
    "                    # loop over packages listed\n",
    "                    for k in range(len(down_up)):\n",
    "\n",
    "                        # if download speed\n",
    "                        if down_up[k] == \"Download\":\n",
    "                            if k != 0:\n",
    "                                count += 1\n",
    "\n",
    "                            # add download speed\n",
    "                            speed_array[count, 0] = speed[k]\n",
    "\n",
    "                        # if upload, add upload speed\n",
    "                        else:\n",
    "                            speed_array[count, 1] = speed[k]\n",
    "\n",
    "                    # select edit option to change address\n",
    "                    edit = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"plans-search\"]/div/div/div/h1/span')))\n",
    "                    edit.click()\n",
    "\n",
    "                    # append to lists\n",
    "                    idxs.append(i)\n",
    "                    all_prices.append(price)\n",
    "                    all_names.append(name)\n",
    "                    all_type_list.append(type_list)\n",
    "                    all_speeds.append(speed_array)\n",
    "\n",
    "                    # set problem counter \n",
    "                    problem_counter = 0\n",
    "            \n",
    "            # increment address counter within while loop\n",
    "            i += 1\n",
    "\n",
    "        # if try fails, throw exception and increment counter (retry until problem_counter hits 5)\n",
    "        # throws error if we try to edit search plans but this is not an option because nothing was searched after hitting home page\n",
    "        except TimeoutException as ex:\n",
    "            problem_counter += 1\n",
    "            \n",
    "            # if 2 problems with address, increment address counter, skip address, and reset problem counter\n",
    "            if problem_counter == 2:\n",
    "                i += 1\n",
    "                print(\"skip\")\n",
    "                problem_counter = 0\n",
    "\n",
    "        # get time taken to run as well as % completetion\n",
    "        mid = time.time()\n",
    "        if i == 1 * int(len(addresses)/10): print(\"10% @ {}\".format(mid - start))\n",
    "        if i == 2 * int(len(addresses)/10): print(\"20% @ {}\".format(mid - start))\n",
    "        if i == 3 * int(len(addresses)/10): print(\"30% @ {}\".format(mid - start))\n",
    "        if i == 4 * int(len(addresses)/10): print(\"40% @ {}\".format(mid - start))\n",
    "        if i == 5 * int(len(addresses)/10): print(\"50% @ {}\".format(mid - start))\n",
    "        if i == 6 * int(len(addresses)/10): print(\"60% @ {}\".format(mid - start))\n",
    "        if i == 7 * int(len(addresses)/10): print(\"70% @ {}\".format(mid - start))\n",
    "        if i == 8 * int(len(addresses)/10): print(\"80% @ {}\".format(mid - start))\n",
    "        if i == 9 * int(len(addresses)/10): print(\"90% @ {}\".format(mid - start))\n",
    "\n",
    "    # close driver\n",
    "    driver.quit()        \n",
    "    \n",
    "    # convert indices to array and get time\n",
    "    idxs = np.array(idxs)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    return all_prices, all_names, all_type_list, all_speeds, idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten list of lists\n",
    "def flatten(t):\n",
    "    '''\n",
    "    Description:\n",
    "        Flattens our lists of lists so that we can make clean dataframe; helper function\n",
    "    \n",
    "    Inputs:\n",
    "        t: list of lists (jagged list)\n",
    "        \n",
    "    Outputs:\n",
    "         np.array([item for sublist in t for item in sublist]): array, flattened array from list of lists\n",
    "    '''\n",
    "    return np.array([item for sublist in t for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(all_prices, all_names, all_type_list, all_speeds, idxs):\n",
    "    '''\n",
    "    Description:\n",
    "        Make dataframe using prices, names, internet types, speeds, and FIPs code at bg, tr, and ct levels\n",
    "    \n",
    "    Inputs:\n",
    "        all_prices: jagged list (list of varying sized lists), package prices\n",
    "        all_names: jagged list (same size as all_prices), package names\n",
    "        all_type_list: jagged list (same size as all_prices), package names\n",
    "        all_speeds: jagged list (same size as all_prices), package names\n",
    "        idxs: array, array of indices where information was successfully scraped (aligns with addresses)\n",
    "        \n",
    "    Outputs:\n",
    "        all_prices: jagged list (list of varying sized lists), package prices\n",
    "        all_names: jagged list (same size as all_prices), package names\n",
    "        all_type_list: jagged list (same size as all_prices), package names\n",
    "        all_speeds: jagged list (same size as all_prices), package names\n",
    "        idxs: array, array of indices where \n",
    "    '''\n",
    "    # get number of packages, valid address, and construct result (the addresses column in the dataframe)\n",
    "    num_packages = np.array([len(all_prices[i]) for i in range(len(all_prices))])\n",
    "    valid_addresses = addresses[idxs]\n",
    "    \n",
    "    # repeat a valid address \"num_packages\" times\n",
    "    result = np.array([valid_addresses[i] for i in range(len(num_packages)) for j in range(num_packages[i])])\n",
    "    \n",
    "    # flatten download and upload arrays\n",
    "    download = flatten([all_speeds[i][:, 0] for i in range(len(all_speeds))])\n",
    "    upload = flatten([all_speeds[i][:, 1] for i in range(len(all_speeds))])\n",
    "\n",
    "    # get block groups, tracts, and counties from addresses data and add to dataframe\n",
    "    short_blockgroup_geoid = block_geoids[idxs]\n",
    "    short_tract_geoid = np.array([x[:11] for x in short_blockgroup_geoid])\n",
    "    short_county_geoid = np.array([x[:5] for x in short_blockgroup_geoid])\n",
    "\n",
    "    # repeat block group, tract, county name \"num_packages\" times for the number of packages within that area (these will be columns in df)\n",
    "    short_blockgroup_geoid2 = np.array([short_blockgroup_geoid[i] for i in range(len(num_packages)) for j in range(num_packages[i])])\n",
    "    short_tract_geoid2 = np.array([short_tract_geoid[i] for i in range(len(num_packages)) for j in range(num_packages[i])])\n",
    "    short_county_geoid2 = np.array([short_county_geoid[i] for i in range(len(num_packages)) for j in range(num_packages[i])])\n",
    "\n",
    "    # final dataframe\n",
    "    df = pd.DataFrame({\"address\": result, \"price\": flatten(all_prices), \"name\": flatten(all_names),\n",
    "                       \"type\": flatten(all_type_list), \"download\": download, \"upload\": upload,\n",
    "                       \"block_group\": short_blockgroup_geoid2, \"tract\": short_tract_geoid2,\n",
    "                       \"county\": short_county_geoid2})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|█████████████████████████████████████████████████████████████████████████| 7.82M/7.82M [00:00<00:00, 25.6MB/s]\n",
      "/var/folders/31/_flrh_251y578sg72lx1yb780000gn/T/ipykernel_55634/2886812516.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "# start driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(\"https://broadbandnow.com/compare/plans\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# set driver params\n",
    "driver_wait = 20\n",
    "sleep_time = 2\n",
    "wait = WebDriverWait(driver, driver_wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'three_address_in_block_group.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run read_and_clean_addresses_for_bgs and get list of addresses, block_geoids\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# NEED: list of addresses with GEOID for block as \"geoid_blk\", mail address as \"mail_address\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# I used Corelogic for housing information and scraped 3 for each block group, although I only use 1\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m addresses, block_geoids \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_clean_addresses_for_bgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthree_address_in_block_group.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mread_and_clean_addresses_for_bgs\u001b[0;34m(data, need_subset, size_subset)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mDescription:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Check existence of xpath on page\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    returns True if xpath exists, False if not\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# read in csv, drop index, and update block column\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m address_sample_3_per_bg \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m address_sample_3_per_bg \u001b[38;5;241m=\u001b[39m address_sample_3_per_bg\u001b[38;5;241m.\u001b[39mreset_index(drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m address_sample_3_per_bg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeoid_blk\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m address_sample_3_per_bg\u001b[38;5;241m.\u001b[39mgeoid_blk\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/sdc.broadband_dev/.venv/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/sdc.broadband_dev/.venv/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/sdc.broadband_dev/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/sdc.broadband_dev/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/GitHub/sdc.broadband_dev/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/sdc.broadband_dev/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Documents/GitHub/sdc.broadband_dev/.venv/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'three_address_in_block_group.csv'"
     ]
    }
   ],
   "source": [
    "# run read_and_clean_addresses_for_bgs and get list of addresses, block_geoids\n",
    "# NEED: list of addresses with GEOID for block as \"geoid_blk\", mail address as \"mail_address\"\n",
    "# I used Corelogic for housing information and scraped 3 for each block group, although I only use 1\n",
    "addresses, block_geoids = read_and_clean_addresses_for_bgs(data = \"three_address_in_block_group.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given addresses in the correct format and driver: gets prices, names, types, speeds, (and indicies where successful)\n",
    "all_prices, all_names, all_type_list, all_speeds, idxs = scrape_prices(driver, addresses[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produces dataframe\n",
    "df = make_df(all_prices, all_names, all_type_list, all_speeds, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
